{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>MaxPartialCharge</th>\n",
       "      <th>MinPartialCharge</th>\n",
       "      <th>MaxAbsPartialCharge</th>\n",
       "      <th>MinAbsPartialCharge</th>\n",
       "      <th>Irritation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.156202</td>\n",
       "      <td>0.917365</td>\n",
       "      <td>1.215465</td>\n",
       "      <td>0.378529</td>\n",
       "      <td>0.466241</td>\n",
       "      <td>0.209837</td>\n",
       "      <td>0.232062</td>\n",
       "      <td>0.536511</td>\n",
       "      <td>0.303403</td>\n",
       "      <td>0.196044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133787</td>\n",
       "      <td>0.361076</td>\n",
       "      <td>0.384876</td>\n",
       "      <td>0.713980</td>\n",
       "      <td>0.694891</td>\n",
       "      <td>0.375455</td>\n",
       "      <td>0.234795</td>\n",
       "      <td>0.536740</td>\n",
       "      <td>0.303088</td>\n",
       "      <td>0.201185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.006071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.841560</td>\n",
       "      <td>0.417235</td>\n",
       "      <td>0.459854</td>\n",
       "      <td>0.277571</td>\n",
       "      <td>0.130574</td>\n",
       "      <td>0.724242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.535903</td>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.648805</td>\n",
       "      <td>0.619348</td>\n",
       "      <td>0.650299</td>\n",
       "      <td>0.287167</td>\n",
       "      <td>0.165618</td>\n",
       "      <td>0.464030</td>\n",
       "      <td>0.396366</td>\n",
       "      <td>0.071052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.577659</td>\n",
       "      <td>0.927894</td>\n",
       "      <td>1.240141</td>\n",
       "      <td>0.480844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250357</td>\n",
       "      <td>0.112937</td>\n",
       "      <td>0.724242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MaxAbsEStateIndex  MinAbsEStateIndex  MinEStateIndex       qed       SPS  \\\n",
       "0          -0.156202           0.917365        1.215465  0.378529  0.466241   \n",
       "1           0.133787           0.361076        0.384876  0.713980  0.694891   \n",
       "2          -2.006071           0.000000        1.841560  0.417235  0.459854   \n",
       "3          -0.535903           0.607914        0.648805  0.619348  0.650299   \n",
       "4          -1.577659           0.927894        1.240141  0.480844  0.000000   \n",
       "\n",
       "      MolWt  MaxPartialCharge  MinPartialCharge  MaxAbsPartialCharge  \\\n",
       "0  0.209837          0.232062          0.536511             0.303403   \n",
       "1  0.375455          0.234795          0.536740             0.303088   \n",
       "2  0.277571          0.130574          0.724242             0.000000   \n",
       "3  0.287167          0.165618          0.464030             0.396366   \n",
       "4  0.250357          0.112937          0.724242             0.000000   \n",
       "\n",
       "   MinAbsPartialCharge  Irritation  \n",
       "0             0.196044           1  \n",
       "1             0.201185           1  \n",
       "2             0.005127           1  \n",
       "3             0.071052           1  \n",
       "4             0.028049           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'selected_features_PCA.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "X = df.drop(columns=['Irritation'])  # Assuming 'Call' is the target variable\n",
    "y = df['Irritation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (353, 10)\n",
      "Shape of X_test: (89, 10)\n",
      "Shape of y_train: (353,)\n",
      "Shape of y_test: (89,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-31 11:13:46,539] A new study created in memory with name: no-name-897a39b5-00ef-4077-ad92-8a8c397ecd8b\n",
      "[I 2025-01-31 11:13:46,666] Trial 0 finished with value: 0.8113207547169812 and parameters: {'booster': 'gbtree', 'lambda': 0.00028651305251997715, 'alpha': 0.018072927516376902, 'learning_rate': 0.03704544185357371, 'n_estimators': 704, 'max_depth': 8, 'min_child_weight': 2, 'gamma': 0.27205219788638607, 'subsample': 0.5889694273600685, 'colsample_bytree': 0.6273792546453507, 'early_stopping_rounds': 89}. Best is trial 0 with value: 0.8113207547169812.\n",
      "[I 2025-01-31 11:13:46,710] Trial 1 finished with value: 0.8301886792452831 and parameters: {'booster': 'gbtree', 'lambda': 0.01799006755619394, 'alpha': 0.0942470725405577, 'learning_rate': 0.08032033284845028, 'n_estimators': 123, 'max_depth': 8, 'min_child_weight': 8, 'gamma': 0.17201533595489002, 'subsample': 0.7580595069790005, 'colsample_bytree': 0.6344462663748307, 'early_stopping_rounds': 26}. Best is trial 1 with value: 0.8301886792452831.\n",
      "[I 2025-01-31 11:13:47,141] Trial 2 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.0009648928612336718, 'alpha': 0.0001412923751739288, 'learning_rate': 0.04768227477089673, 'n_estimators': 572, 'max_depth': 6, 'min_child_weight': 1, 'gamma': 0.0008619429428953677, 'subsample': 0.7833742768257677, 'colsample_bytree': 0.859096471450917, 'early_stopping_rounds': 54}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:47,251] Trial 3 finished with value: 0.8113207547169812 and parameters: {'booster': 'gbtree', 'lambda': 0.0023719679107451954, 'alpha': 0.5539462066428498, 'learning_rate': 0.01935095389918889, 'n_estimators': 330, 'max_depth': 7, 'min_child_weight': 14, 'gamma': 8.070917994531336e-05, 'subsample': 0.5828880179336174, 'colsample_bytree': 0.6611751276414941, 'early_stopping_rounds': 41}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:47,323] Trial 4 finished with value: 0.7735849056603774 and parameters: {'booster': 'gbtree', 'lambda': 3.5733026149823144, 'alpha': 0.18128472032665316, 'learning_rate': 0.12839306280131393, 'n_estimators': 323, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 7.195204893420341e-05, 'subsample': 0.6932470897811625, 'colsample_bytree': 0.7148187854970707, 'early_stopping_rounds': 72}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:47,381] Trial 5 finished with value: 0.8113207547169812 and parameters: {'booster': 'gbtree', 'lambda': 0.3323310391352884, 'alpha': 0.000285076138918558, 'learning_rate': 0.19200509997144807, 'n_estimators': 541, 'max_depth': 8, 'min_child_weight': 8, 'gamma': 1.3296090006118201e-07, 'subsample': 0.7403428117884173, 'colsample_bytree': 0.8457397274177025, 'early_stopping_rounds': 78}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:54,684] Trial 6 finished with value: 0.8301886792452831 and parameters: {'booster': 'dart', 'lambda': 0.00018254635167845828, 'alpha': 7.361285200357653, 'learning_rate': 0.15976187090364932, 'n_estimators': 474, 'max_depth': 5, 'min_child_weight': 14, 'gamma': 0.08503950633915049, 'subsample': 0.6890959629681065, 'colsample_bytree': 0.6561919940300047, 'early_stopping_rounds': 49}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:54,864] Trial 7 finished with value: 0.8301886792452831 and parameters: {'booster': 'dart', 'lambda': 2.7119823590481342, 'alpha': 0.028462384663162206, 'learning_rate': 0.11865893960950834, 'n_estimators': 473, 'max_depth': 7, 'min_child_weight': 6, 'gamma': 6.333762623964652e-05, 'subsample': 0.8751757032157623, 'colsample_bytree': 0.7169910492682146, 'early_stopping_rounds': 10}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:54,920] Trial 8 finished with value: 0.7924528301886793 and parameters: {'booster': 'gbtree', 'lambda': 0.0028880258959112404, 'alpha': 0.4359924033157286, 'learning_rate': 0.07399013550841964, 'n_estimators': 251, 'max_depth': 4, 'min_child_weight': 13, 'gamma': 0.2109515780778427, 'subsample': 0.6198204935959272, 'colsample_bytree': 0.6884851600041983, 'early_stopping_rounds': 57}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:54,982] Trial 9 finished with value: 0.8113207547169812 and parameters: {'booster': 'gbtree', 'lambda': 8.552258220708895, 'alpha': 0.00028344722311558437, 'learning_rate': 0.11720592902152868, 'n_estimators': 640, 'max_depth': 8, 'min_child_weight': 15, 'gamma': 9.062336299989236e-08, 'subsample': 0.7068709018955233, 'colsample_bytree': 0.5130559400809696, 'early_stopping_rounds': 86}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:56,567] Trial 10 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.04075835662835196, 'alpha': 0.0024346755738383348, 'learning_rate': 0.02557998250136945, 'n_estimators': 794, 'max_depth': 3, 'min_child_weight': 4, 'gamma': 0.0040667913232455, 'subsample': 0.8559570079039459, 'colsample_bytree': 0.8937117054016117, 'early_stopping_rounds': 63}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:57,117] Trial 11 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.017379680452125942, 'alpha': 0.002918635211797288, 'learning_rate': 0.054090835673663254, 'n_estimators': 121, 'max_depth': 10, 'min_child_weight': 10, 'gamma': 0.003550763486335083, 'subsample': 0.796410434063352, 'colsample_bytree': 0.7977907892426853, 'early_stopping_rounds': 26}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:13:57,665] Trial 12 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.002416830946848322, 'alpha': 0.0022060709622730053, 'learning_rate': 0.046584838576012734, 'n_estimators': 123, 'max_depth': 10, 'min_child_weight': 11, 'gamma': 0.0021303855998923383, 'subsample': 0.8015466153247229, 'colsample_bytree': 0.800789375407823, 'early_stopping_rounds': 33}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:11,537] Trial 13 finished with value: 0.8301886792452831 and parameters: {'booster': 'dart', 'lambda': 0.02235082399421007, 'alpha': 0.002120511702755371, 'learning_rate': 0.010705951270995693, 'n_estimators': 576, 'max_depth': 5, 'min_child_weight': 11, 'gamma': 3.655951348159374e-06, 'subsample': 0.8105717618761954, 'colsample_bytree': 0.7779581411525549, 'early_stopping_rounds': 16}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:12,261] Trial 14 finished with value: 0.8301886792452831 and parameters: {'booster': 'dart', 'lambda': 0.17719087417039095, 'alpha': 0.00011089920925583903, 'learning_rate': 0.05800369586662717, 'n_estimators': 370, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 3.7115179558695077, 'subsample': 0.8101645246652124, 'colsample_bytree': 0.8867023953921456, 'early_stopping_rounds': 26}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:14,087] Trial 15 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.0007420006191604747, 'alpha': 0.0007328150340106173, 'learning_rate': 0.03259911419710627, 'n_estimators': 227, 'max_depth': 9, 'min_child_weight': 10, 'gamma': 0.0032543147896033835, 'subsample': 0.5228947686549758, 'colsample_bytree': 0.7903606569634636, 'early_stopping_rounds': 43}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:14,545] Trial 16 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.007596641713348554, 'alpha': 0.007034694916071063, 'learning_rate': 0.062400958596362195, 'n_estimators': 706, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.0014637769321562662, 'subsample': 0.7652624849700647, 'colsample_bytree': 0.8362448374156771, 'early_stopping_rounds': 65}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:15,379] Trial 17 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.11574248336876664, 'alpha': 0.00012655737834893572, 'learning_rate': 0.019782590823894584, 'n_estimators': 564, 'max_depth': 5, 'min_child_weight': 6, 'gamma': 2.224933364622351e-06, 'subsample': 0.8862221908577425, 'colsample_bytree': 0.755495658594773, 'early_stopping_rounds': 6}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:16,045] Trial 18 finished with value: 0.8301886792452831 and parameters: {'booster': 'dart', 'lambda': 0.0007810816280025795, 'alpha': 0.0009007170643286391, 'learning_rate': 0.044185057853860586, 'n_estimators': 410, 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.020253396378927393, 'subsample': 0.8440733245166037, 'colsample_bytree': 0.8407082899536954, 'early_stopping_rounds': 34}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:17,984] Trial 19 finished with value: 0.8301886792452831 and parameters: {'booster': 'dart', 'lambda': 0.007435171908568398, 'alpha': 0.00900971213792476, 'learning_rate': 0.013621560521693778, 'n_estimators': 215, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 5.105892044341855e-06, 'subsample': 0.7239993666305907, 'colsample_bytree': 0.537844690147788, 'early_stopping_rounds': 97}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:20,182] Trial 20 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.0007927049755662761, 'alpha': 0.0005591829832281768, 'learning_rate': 0.08642871368854806, 'n_estimators': 524, 'max_depth': 6, 'min_child_weight': 12, 'gamma': 2.4257562980606777, 'subsample': 0.793558034092168, 'colsample_bytree': 0.7549393315816328, 'early_stopping_rounds': 55}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:21,474] Trial 21 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.20767533014151507, 'alpha': 0.0001052829515870427, 'learning_rate': 0.023361172901620406, 'n_estimators': 611, 'max_depth': 5, 'min_child_weight': 7, 'gamma': 1.0508579920373444e-08, 'subsample': 0.8975419160656402, 'colsample_bytree': 0.7472165911141503, 'early_stopping_rounds': 7}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:23,391] Trial 22 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.05343124738384906, 'alpha': 0.00026071988106547304, 'learning_rate': 0.016823833787492196, 'n_estimators': 675, 'max_depth': 4, 'min_child_weight': 1, 'gamma': 5.3060025464385e-06, 'subsample': 0.843114684034334, 'colsample_bytree': 0.8180133778565621, 'early_stopping_rounds': 18}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:23,804] Trial 23 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.08852886731677817, 'alpha': 0.0040321639231769135, 'learning_rate': 0.03152903489564176, 'n_estimators': 512, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.0003410034221823603, 'subsample': 0.8951733020708253, 'colsample_bytree': 0.8686823302992094, 'early_stopping_rounds': 5}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:24,647] Trial 24 finished with value: 0.8301886792452831 and parameters: {'booster': 'dart', 'lambda': 0.30839669038360634, 'alpha': 0.0009860630150299813, 'learning_rate': 0.05211008107320759, 'n_estimators': 413, 'max_depth': 7, 'min_child_weight': 9, 'gamma': 0.0003671700690434577, 'subsample': 0.6647639433613876, 'colsample_bytree': 0.7569836757295897, 'early_stopping_rounds': 20}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:28,274] Trial 25 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 1.1151578743399964, 'alpha': 0.00010284572209492725, 'learning_rate': 0.026894734632977087, 'n_estimators': 776, 'max_depth': 5, 'min_child_weight': 5, 'gamma': 9.571363658422735e-07, 'subsample': 0.7783409510777503, 'colsample_bytree': 0.5811428183693035, 'early_stopping_rounds': 46}. Best is trial 2 with value: 0.8490566037735849.\n",
      "[I 2025-01-31 11:14:28,850] Trial 26 finished with value: 0.8679245283018868 and parameters: {'booster': 'dart', 'lambda': 0.8180748125197252, 'alpha': 0.00032486338970744215, 'learning_rate': 0.0392286456084305, 'n_estimators': 596, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 1.9688685252630635e-05, 'subsample': 0.8307903391602734, 'colsample_bytree': 0.8118477702802908, 'early_stopping_rounds': 13}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:30,233] Trial 27 finished with value: 0.7924528301886793 and parameters: {'booster': 'dart', 'lambda': 0.007993371336013704, 'alpha': 0.0007517175686856412, 'learning_rate': 0.03647385369194008, 'n_estimators': 608, 'max_depth': 6, 'min_child_weight': 10, 'gamma': 2.352151474219457e-05, 'subsample': 0.8262900252400363, 'colsample_bytree': 0.8630671957442952, 'early_stopping_rounds': 37}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:30,984] Trial 28 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.8267526856013179, 'alpha': 0.0094821947377898, 'learning_rate': 0.06609690668384674, 'n_estimators': 732, 'max_depth': 9, 'min_child_weight': 12, 'gamma': 0.030289257201696344, 'subsample': 0.7419062662049699, 'colsample_bytree': 0.8136114328052779, 'early_stopping_rounds': 30}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:31,421] Trial 29 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.7265890286837686, 'alpha': 0.0003575211963376608, 'learning_rate': 0.04261908771647712, 'n_estimators': 661, 'max_depth': 7, 'min_child_weight': 2, 'gamma': 0.013685434126254283, 'subsample': 0.6566940793364109, 'colsample_bytree': 0.8668477612577932, 'early_stopping_rounds': 12}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:31,660] Trial 30 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.00024125526994662523, 'alpha': 0.0017886627986068774, 'learning_rate': 0.09522585956878689, 'n_estimators': 422, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 0.0005912091404904055, 'subsample': 0.7827103445801049, 'colsample_bytree': 0.7810802548019226, 'early_stopping_rounds': 23}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:32,569] Trial 31 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.10697313215109382, 'alpha': 0.00017724447347958013, 'learning_rate': 0.037373822264914654, 'n_estimators': 569, 'max_depth': 5, 'min_child_weight': 7, 'gamma': 1.62053258300026e-05, 'subsample': 0.8626299040631126, 'colsample_bytree': 0.7338614169886362, 'early_stopping_rounds': 12}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:34,298] Trial 32 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 0.014118607627813903, 'alpha': 0.00022948534026198854, 'learning_rate': 0.017844445830389716, 'n_estimators': 595, 'max_depth': 4, 'min_child_weight': 4, 'gamma': 9.124564359064657e-07, 'subsample': 0.8306170756274578, 'colsample_bytree': 0.8215075584327055, 'early_stopping_rounds': 14}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,221] Trial 33 finished with value: 0.7924528301886793 and parameters: {'booster': 'dart', 'lambda': 0.00011070836302696564, 'alpha': 0.0712938362017484, 'learning_rate': 0.05122619720564124, 'n_estimators': 479, 'max_depth': 7, 'min_child_weight': 9, 'gamma': 1.1848129007296141e-06, 'subsample': 0.8829369378948317, 'colsample_bytree': 0.7685468533315942, 'early_stopping_rounds': 24}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,322] Trial 34 finished with value: 0.8490566037735849 and parameters: {'booster': 'gbtree', 'lambda': 0.09604977031770709, 'alpha': 0.0004515710600342651, 'learning_rate': 0.014171116079511993, 'n_estimators': 544, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.00012690070194912951, 'subsample': 0.7578157445993181, 'colsample_bytree': 0.7968386669526241, 'early_stopping_rounds': 5}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,389] Trial 35 finished with value: 0.8679245283018868 and parameters: {'booster': 'gbtree', 'lambda': 1.9021323363997609, 'alpha': 0.0012707790241839525, 'learning_rate': 0.02165131077629935, 'n_estimators': 112, 'max_depth': 5, 'min_child_weight': 8, 'gamma': 1.8658777217611213e-05, 'subsample': 0.8213982848670732, 'colsample_bytree': 0.7002524657679992, 'early_stopping_rounds': 41}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,454] Trial 36 finished with value: 0.8490566037735849 and parameters: {'booster': 'gbtree', 'lambda': 3.838775097287418, 'alpha': 0.0042877852222066035, 'learning_rate': 0.030318829064736985, 'n_estimators': 146, 'max_depth': 10, 'min_child_weight': 8, 'gamma': 2.6739267339263587e-05, 'subsample': 0.8264939136743336, 'colsample_bytree': 0.629558785096355, 'early_stopping_rounds': 38}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,513] Trial 37 finished with value: 0.8301886792452831 and parameters: {'booster': 'gbtree', 'lambda': 1.44079307305496, 'alpha': 0.0015521723404698727, 'learning_rate': 0.03880190914028921, 'n_estimators': 103, 'max_depth': 7, 'min_child_weight': 10, 'gamma': 0.0009221838803637695, 'subsample': 0.7525451626952937, 'colsample_bytree': 0.6881148115786596, 'early_stopping_rounds': 30}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,590] Trial 38 finished with value: 0.7924528301886793 and parameters: {'booster': 'gbtree', 'lambda': 9.54994507224387, 'alpha': 7.231062984338165, 'learning_rate': 0.021494095018204096, 'n_estimators': 173, 'max_depth': 8, 'min_child_weight': 8, 'gamma': 0.00017681216928515356, 'subsample': 0.7230206675903467, 'colsample_bytree': 0.6607729935493133, 'early_stopping_rounds': 61}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,672] Trial 39 finished with value: 0.8113207547169812 and parameters: {'booster': 'gbtree', 'lambda': 2.178226207342857, 'alpha': 0.01781064974822279, 'learning_rate': 0.07675901773660562, 'n_estimators': 337, 'max_depth': 5, 'min_child_weight': 11, 'gamma': 0.008175959513073316, 'subsample': 0.7720832109935323, 'colsample_bytree': 0.5899920532224778, 'early_stopping_rounds': 52}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,752] Trial 40 finished with value: 0.8113207547169812 and parameters: {'booster': 'gbtree', 'lambda': 0.4945770269808795, 'alpha': 0.0011586292073505232, 'learning_rate': 0.09641155234616795, 'n_estimators': 261, 'max_depth': 7, 'min_child_weight': 2, 'gamma': 0.0699067165255472, 'subsample': 0.7947270428088835, 'colsample_bytree': 0.7194445143383916, 'early_stopping_rounds': 75}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:35,873] Trial 41 finished with value: 0.8490566037735849 and parameters: {'booster': 'gbtree', 'lambda': 0.4051598832937483, 'alpha': 0.00018942576025134683, 'learning_rate': 0.020264482282075003, 'n_estimators': 189, 'max_depth': 5, 'min_child_weight': 6, 'gamma': 1.62400980659692e-07, 'subsample': 0.8682738994566839, 'colsample_bytree': 0.8493743016553483, 'early_stopping_rounds': 47}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:38,475] Trial 42 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 4.036639202432684, 'alpha': 0.0004443456619057211, 'learning_rate': 0.02832931932526469, 'n_estimators': 300, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 6.658977160862652e-05, 'subsample': 0.8519433448051754, 'colsample_bytree': 0.6799466604492448, 'early_stopping_rounds': 20}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:38,649] Trial 43 finished with value: 0.8301886792452831 and parameters: {'booster': 'gbtree', 'lambda': 0.0035923525730389982, 'alpha': 0.0001547260405184864, 'learning_rate': 0.010378108494320776, 'n_estimators': 506, 'max_depth': 4, 'min_child_weight': 9, 'gamma': 2.5804100413646305e-06, 'subsample': 0.8300476087132674, 'colsample_bytree': 0.7053848066291658, 'early_stopping_rounds': 42}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:44,156] Trial 44 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.028566896062029622, 'alpha': 0.00035469758096411537, 'learning_rate': 0.015075311141219253, 'n_estimators': 635, 'max_depth': 5, 'min_child_weight': 6, 'gamma': 1.0463491217250339e-05, 'subsample': 0.873899459809512, 'colsample_bytree': 0.7352740900300413, 'early_stopping_rounds': 70}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:46,065] Trial 45 finished with value: 0.8490566037735849 and parameters: {'booster': 'dart', 'lambda': 0.0012536853106051005, 'alpha': 2.5562224549611336, 'learning_rate': 0.0239711468587362, 'n_estimators': 573, 'max_depth': 6, 'min_child_weight': 8, 'gamma': 3.407438521796191e-07, 'subsample': 0.8139985136628152, 'colsample_bytree': 0.8010834827421045, 'early_stopping_rounds': 9}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:46,180] Trial 46 finished with value: 0.8113207547169812 and parameters: {'booster': 'gbtree', 'lambda': 5.5937022543385035, 'alpha': 0.0038660995842433993, 'learning_rate': 0.05431284239307857, 'n_estimators': 450, 'max_depth': 8, 'min_child_weight': 4, 'gamma': 4.2710229123701e-05, 'subsample': 0.8471412703829099, 'colsample_bytree': 0.7682712058260517, 'early_stopping_rounds': 27}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:46,829] Trial 47 finished with value: 0.8113207547169812 and parameters: {'booster': 'dart', 'lambda': 1.64641243811411, 'alpha': 0.0622958622610015, 'learning_rate': 0.06487236057450443, 'n_estimators': 137, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 4.30385534482122e-08, 'subsample': 0.5966028193702865, 'colsample_bytree': 0.8300999130024515, 'early_stopping_rounds': 51}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:47,808] Trial 48 finished with value: 0.7924528301886793 and parameters: {'booster': 'dart', 'lambda': 0.19489285280189197, 'alpha': 0.0006207055852784452, 'learning_rate': 0.011884251568284716, 'n_estimators': 162, 'max_depth': 5, 'min_child_weight': 15, 'gamma': 1.053461960520041e-05, 'subsample': 0.8078933453672477, 'colsample_bytree': 0.887482938585004, 'early_stopping_rounds': 15}. Best is trial 26 with value: 0.8679245283018868.\n",
      "[I 2025-01-31 11:14:52,936] Trial 49 finished with value: 0.7924528301886793 and parameters: {'booster': 'dart', 'lambda': 0.054108520171376004, 'alpha': 0.0012114805145425993, 'learning_rate': 0.033482648482326675, 'n_estimators': 543, 'max_depth': 6, 'min_child_weight': 12, 'gamma': 0.003070523615837247, 'subsample': 0.5309801984376794, 'colsample_bytree': 0.7300644826133892, 'early_stopping_rounds': 58}. Best is trial 26 with value: 0.8679245283018868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'booster': 'dart', 'lambda': 0.8180748125197252, 'alpha': 0.00032486338970744215, 'learning_rate': 0.0392286456084305, 'n_estimators': 596, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 1.9688685252630635e-05, 'subsample': 0.8307903391602734, 'colsample_bytree': 0.8118477702802908, 'early_stopping_rounds': 13}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have at least 1 validation dataset for early stopping.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     50\u001b[0m final_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params, use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m     54\u001b[0m y_pred_final \u001b[38;5;241m=\u001b[39m final_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/sklearn.py:1519\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1491\u001b[0m (\n\u001b[1;32m   1492\u001b[0m     model,\n\u001b[1;32m   1493\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1499\u001b[0m )\n\u001b[1;32m   1500\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1501\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1502\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1517\u001b[0m )\n\u001b[0;32m-> 1519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    185\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py:241\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    239\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 241\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py:241\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    239\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 241\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py:426\u001b[0m, in \u001b[0;36mEarlyStopping.after_iteration\u001b[0;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[1;32m    424\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have at least 1 validation dataset for early stopping.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(evals_log\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Get data name\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata:\n",
      "\u001b[0;31mValueError\u001b[0m: Must have at least 1 validation dataset for early stopping."
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    param = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-4, 10.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 800),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 15),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 5.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.9),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),\n",
    "        \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 5, 100),  # Moved here!\n",
    "    }\n",
    "\n",
    "    # Split train data into training and validation for early stopping\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "    # Train model with early stopping (now in constructor)\n",
    "    model = xgb.XGBClassifier(**param, use_label_encoder=False)\n",
    "    model.fit(\n",
    "        X_train_, y_train_,\n",
    "        eval_set=[(X_valid_, y_valid_)],  # Use validation data\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_pred = model.predict(X_valid_)\n",
    "    accuracy = accuracy_score(y_valid_, y_pred)\n",
    "\n",
    "    return accuracy  # Optuna will maximize this\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_trial.params\n",
    "final_model = xgb.XGBClassifier(**best_params, use_label_encoder=False)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "print(\"Final Model Accuracy:\", accuracy_score(y_test, y_pred_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict, learning_curve, validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 0.8539325842696629\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 50 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n50 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/sklearn.py\", line 1519, in fit\n    self._Booster = train(\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/training.py\", line 182, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py\", line 241, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py\", line 241, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py\", line 426, in after_iteration\n    raise ValueError(msg)\nValueError: Must have at least 1 validation dataset for early stopping.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Model Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test, y_pred_final))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 1. Learning Curves with Early Stopping\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m train_sizes, train_scores, validation_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m train_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m train_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(train_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:1984\u001b[0m, in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         train_test_proportions\u001b[38;5;241m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[1;32m   1965\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m   1966\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m   1967\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m train_test_proportions\n\u001b[1;32m   1983\u001b[0m )\n\u001b[0;32m-> 1984\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1985\u001b[0m results \u001b[38;5;241m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[1;32m   1986\u001b[0m train_scores \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_unique_ticks)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m     )\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 50 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n50 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/sklearn.py\", line 1519, in fit\n    self._Booster = train(\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/training.py\", line 182, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py\", line 241, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py\", line 241, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n  File \"/Users/monika/Library/Python/3.9/lib/python/site-packages/xgboost/callback.py\", line 426, in after_iteration\n    raise ValueError(msg)\nValueError: Must have at least 1 validation dataset for early stopping.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "\n",
    "\n",
    "# Get best hyperparameters from study\n",
    "best_params = study.best_trial.params\n",
    "# Split train data into training and validation for early stopping\n",
    "X_train_, X_valid_, y_train_, y_valid_ = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "# Initialize the model with best parameters and early stopping\n",
    "final_model = xgb.XGBClassifier(**best_params, use_label_encoder=False)\n",
    "\n",
    "final_model.fit(\n",
    "        X_train_, y_train_,\n",
    "        eval_set=[(X_valid_, y_valid_)],  # Use validation data\n",
    "        verbose=False\n",
    ")\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "print(\"Final Model Accuracy:\", accuracy_score(y_test, y_pred_final))\n",
    "\n",
    "# 1. Learning Curves with Early Stopping\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    final_model, X_train, y_train, cv=5, n_jobs=-1, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "validation_mean = np.mean(validation_scores, axis=1)\n",
    "validation_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "# Plot Learning Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training accuracy', color='blue')\n",
    "plt.plot(train_sizes, validation_mean, label='Validation accuracy', color='green')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)\n",
    "plt.fill_between(train_sizes, validation_mean - validation_std, validation_mean + validation_std, color='green', alpha=0.2)\n",
    "plt.title('Learning Curve (Training vs Validation Accuracy)')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate accuracy on test data\n",
    "test_accuracy = final_model.score(X_test, y_test)\n",
    "train_accuracy = final_model.score(X_train, y_train)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_trial: {'booster': 'gbtree', 'lambda': 0.0052103621382832265, 'alpha': 0.1620577811386946, 'learning_rate': 0.1170210607506638, 'n_estimators': 327, 'max_depth': 4, 'min_child_weight': 7, 'gamma': 1.6589810670976416e-05, 'subsample': 0.7843052779851918, 'colsample_bytree': 0.5079409289423329}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_trial: {'booster': 'dart', 'lambda': 0.020998703887689537, 'alpha': 0.1390447886551301, 'learning_rate': 0.11229332078448975, 'n_estimators': 559, 'max_depth': 4, 'min_child_weight': 5, 'gamma': 2.0219692829063354e-05, 'subsample': 0.7175177317110472, 'colsample_bytree': 0.8899318411369608}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_trial: {'booster': 'dart', 'lambda': 3.9815594987137994, 'alpha': 1.1692306809315036, 'learning_rate': 0.08046698602150275, 'n_estimators': 700, 'max_depth': 6, 'min_child_weight': 10, 'gamma': 2.123245273357147e-05, 'subsample': 0.7970691270411416, 'colsample_bytree': 0.6813800155627162}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp[\"n_estimators\"]=500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
